---
header-includes:
    - \usepackage{needspace}
    - \usepackage{float}
    - \floatplacement{figure}{H}
title: "Sprawozdanie Struktury Baz Danych Projekt 1"
author: "Sebastian Kwaśniak"
date: "`r Sys.Date()`"
geometry: margin=2cm
output:
    pdf_document:
        keep_tex: yes
        extra_dependencies: ["float"]
---

```{r include = FALSE}
knitr::opts_chunk$set(fig.pos = "!H", out.extra = "")
```

\renewcommand{\figurename}{Rys.}

# Wprowadzenie
Wylosowane przeze mnie typ rekordu to:

>  29. File records: Right circular cylinders - the radius of the base and the
>  height of the cylinder. Sorting by volume.

Implementacja w języku C++. Przyjąłem, że jeden rekord jest podzielony na cztery liczby, rozmiar rekordu
to 16 bajtów (4 bajty dla klucza, 4 bajty dla podstawy, 4 bajty dla wysokości,
4 bajty dla wskaźnika).

Zastosowałem optymalizację polegającą na nie alokowaniu całego obszaru indeksu na raz,
rozmiar dostosowuje się do zajętości miejsca. Nie ma to wpływu na algorytm,
a głównie na zajętość pamięci.

# Opis struktury kodu

Kod został głównie przeniesiony z projektu 1, w którym:

- Klasa `Tape` zajmuje się obsługą zarówno głównej taśmy oraz przepełnienia
- Klasa `Index` zajmuje się trzymaniem indeksów
- Klasa `Cylinder` implementuje typ rekordu

# Zasada działania

## Łańcuch przepełnień

Łańcuch przepełnień działa na zasadzie podobnej do struktury `linked list`,
gdzie w moim wypadku, wskaźnikami jest offset w pliku dodany o 1
(wartość 0 jest u mnie wartością specjalną - wskaźnik nie istnieje).

## Insert

Gdy próbujemy umieścić nowy rekord w taśmie, najpierw przeszukujemy index.
Index posiada w sobie informacje na której stronie zaczynają się
poszczególne klucze, dlatego wystarczy że znajdziemy poprzednika od
pierwszego większego znalezionego klucza od tego który chcemy wstawić.
Mając stronę, nie musimy przeszukiwać całego pliku a tylko skoczyć do wybranej
strony i odczytać ją. W niej szukamy poprzednika i umieszczamy go zaraz
po poprzedniku. Jeśli nie ma miejsca w głównej taśme, to umieszczamy
go w łańcuchu przepełnień.

## Reorganise

1. Tworzymy dwa tymczasowe pliki: dodatkową taśmę i indeks, ze wzoru niżej
wyliczamy liczbę stron głównych, gdzie $N,V$ - liczba rekordów w taśme głównej
i przepełnieniu, $b$ - liczba rekordów danch na stronę, $\alpha$ - średnie
zapełnienie strony po reorganizacji pliku.

\begin{align}
\lceil \frac{N+V}{b* \alpha }\rceil
\end{align}

2. Przechodizmy kolejno przez rekordy zgodnie z rośnięciem kluczy i umieszczamy
je na kolejnyhc stronach (respektując $\alpha$)

3. Usuwamy stare pliki i zamieniamy tymczasowe na nie.


# Prezentacja wyników programu

Po włączeniu programu użytkownikowi zostają pokazane wszystkie możliwości:

TODO: przekopiować output tutaj

Głównie są to 2 komendy:

- `insert <key> <base> <height>` - dodanie rekordu do bazy
- `file` - wczytanie komend z pliku (domyślnie plik z nazwą `input.txt`)
- `dump` - wypisanie całej bazy
- `reorganise` - reorganizacja całej bazy

Przykładowe wyjście z programu:

TODO: przykładowe wyjście

# Eksperyment

Przeprowadzono eksperymenty na zasadzie wczytywania danych z pliku, gdzie zostało 
wygenerowanych 1000 operacji `insert`.

- Ilość rekordów przetrzymywanych w głównej taśmie przyjąłem jako 4
- Rozmiar jednego rekordu w głównej taśmie lub obszarze przepełnienia to 16 bajtów
- Rozmiar jednego rekordu w indeksie to 16 bajtów

```{r, echo=FALSE}
data <- read.csv("filesizes.csv", header = TRUE, check.names = FALSE)
data$Suma <- (data$"rozmiar indeksu" + data$"rozmiar obszaru głównego" + data$"rozmiar obszaru przepełnienia")
library(knitr)
library(ggplot2)
kable(data, align = "c")

ggplot(data, aes(x = N, y = Suma)) +
    geom_point(color = "blue", size = 3) +
    geom_line(color = "red", linetype = "dashed") +
    labs(
        title = "Rozmiar pliku dla różnych ilości rekordów",
        x = "N",
        y = "Rozmiar pliku"
    ) +
    scale_y_continuous(labels = scales::label_comma()) +
    theme_minimal()
```

Jak widzimy rozmiar plików rośnie liniowo. Do tego widać zależność rośnięcia plików blokowo.


```{r, echo=FALSE}
data <- read.csv("alpha.csv", header = TRUE, check.names = FALSE)
data$Suma <- (data$"rozmiar indeksu" + data$"rozmiar obszaru głównego" + data$"rozmiar obszaru przepełnienia")
library(knitr)
library(latex2exp)
library(ggplot2)
kable(data, align = "c")

ggplot(data, aes(x = alpha, y = Suma)) +
    geom_point(color = "blue", size = 3) +
    geom_line(color = "red", linetype = "dashed") +
    labs(
        title = TeX("Rozmiar pliku dla różnych $\\alpha$"),
        x = TeX("$\\alpha$"),
        y = "Rozmiar pliku"
    ) +
    scale_y_continuous(labels = scales::label_comma()) +
    theme_minimal()
```

Widzimy, że spadek rozmiaru pliku jes gwałtowny pomiędzy $\alpha=[0.3;0.4]$
oraz później tendencja jest w mniejszym stopniu. To znaczy, że reorganizacja tworzy mniej stron,
przez co będą pełniejsze, ale reorganizacja będzie musiała następować częściej.


W następnym eksperymencie, dla lepszego zobrazowania danych zmieniłem wielkość bloku na 10 rekordów.

```{r, echo=FALSE}
data <- read.csv("operations.csv", header = TRUE, check.names = FALSE)
data$Suma <- (data$"śr. odczyty" + data$"śr. zapisy")
library(knitr)
library(latex2exp)
library(ggplot2)
kable(data, align = "c")

ggplot(data, aes(x = alpha, y = Suma)) +
    geom_point(color = "blue", size = 3) +
    geom_line(color = "red", linetype = "dashed") +
    labs(
        title = TeX("Ilość operacji dla różnych $\\alpha$"),
        x = TeX("$\\alpha$"),
        y = "Ilość operacji"
    ) +
    scale_y_continuous(labels = scales::label_comma()) +
    theme_minimal()
```

Widzimy że operacje oscylują w okolicy liczby $6$. Zapisy zawsze są blisko $2$,
a odczyty blisko $4$. Ilość odczytów głównie zależy od
tego jak bardzo porozrzucane są następne elementy
w łańcuchu przepełnień, na co głównie wpływ ma duża losowość.

# Podsumowanie

Projekt pozwolił na zrozumienie organizacji indeksowej pliku i dlaczego
indeksowanie pozwala znacząco przyspieszyć wszystkie typy operacji,
oraz korzyści płynące z posiadania indeksu. Na bazie eksperymentów,
można zuważyć zależności pomiędzy $\alpha$ oraz wielkością pliku i ilością
operacji.
